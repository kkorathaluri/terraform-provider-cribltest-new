// Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
	"github.com/speakeasy/terraform-provider-cribl-terraform/internal/sdk/internal/utils"
)

type OutputAzureDataExplorerType string

const (
	OutputAzureDataExplorerTypeAzureDataExplorer OutputAzureDataExplorerType = "azure_data_explorer"
)

func (e OutputAzureDataExplorerType) ToPointer() *OutputAzureDataExplorerType {
	return &e
}
func (e *OutputAzureDataExplorerType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_data_explorer":
		*e = OutputAzureDataExplorerType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerType: %v", v)
	}
}

// IngestionMode - Method to use for ingesting data.
type IngestionMode string

const (
	IngestionModeBatching  IngestionMode = "batching"
	IngestionModeStreaming IngestionMode = "streaming"
)

func (e IngestionMode) ToPointer() *IngestionMode {
	return &e
}
func (e *IngestionMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "batching":
		fallthrough
	case "streaming":
		*e = IngestionMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for IngestionMode: %v", v)
	}
}

// AzureADAuthenticationEndpoint - Endpoint used to acquire authentication tokens from Azure.
type AzureADAuthenticationEndpoint string

const (
	AzureADAuthenticationEndpointHTTPSLoginMicrosoftonlineCom       AzureADAuthenticationEndpoint = "https://login.microsoftonline.com"
	AzureADAuthenticationEndpointHTTPSLoginMicrosoftonlineUs        AzureADAuthenticationEndpoint = "https://login.microsoftonline.us"
	AzureADAuthenticationEndpointHTTPSLoginPartnerMicrosoftonlineCn AzureADAuthenticationEndpoint = "https://login.partner.microsoftonline.cn"
)

func (e AzureADAuthenticationEndpoint) ToPointer() *AzureADAuthenticationEndpoint {
	return &e
}
func (e *AzureADAuthenticationEndpoint) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "https://login.microsoftonline.com":
		fallthrough
	case "https://login.microsoftonline.us":
		fallthrough
	case "https://login.partner.microsoftonline.cn":
		*e = AzureADAuthenticationEndpoint(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AzureADAuthenticationEndpoint: %v", v)
	}
}

// OutputAzureDataExplorerAuthenticationMethod - The type of OAuth 2.0 client credentials grant flow to use.
type OutputAzureDataExplorerAuthenticationMethod string

const (
	OutputAzureDataExplorerAuthenticationMethodClientSecret     OutputAzureDataExplorerAuthenticationMethod = "clientSecret"
	OutputAzureDataExplorerAuthenticationMethodClientTextSecret OutputAzureDataExplorerAuthenticationMethod = "clientTextSecret"
	OutputAzureDataExplorerAuthenticationMethodCertificate      OutputAzureDataExplorerAuthenticationMethod = "certificate"
)

func (e OutputAzureDataExplorerAuthenticationMethod) ToPointer() *OutputAzureDataExplorerAuthenticationMethod {
	return &e
}
func (e *OutputAzureDataExplorerAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "clientSecret":
		fallthrough
	case "clientTextSecret":
		fallthrough
	case "certificate":
		*e = OutputAzureDataExplorerAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerAuthenticationMethod: %v", v)
	}
}

type OutputAzureDataExplorerCertificate struct {
	// The certificate you registered as credentials for your app in the Azure portal.
	CertificateName *string `json:"certificateName,omitempty"`
}

func (o *OutputAzureDataExplorerCertificate) GetCertificateName() *string {
	if o == nil {
		return nil
	}
	return o.CertificateName
}

// OutputAzureDataExplorerBackpressureBehavior - Whether to block, drop, or queue events when all receivers are exerting backpressure.
type OutputAzureDataExplorerBackpressureBehavior string

const (
	OutputAzureDataExplorerBackpressureBehaviorBlock OutputAzureDataExplorerBackpressureBehavior = "block"
	OutputAzureDataExplorerBackpressureBehaviorDrop  OutputAzureDataExplorerBackpressureBehavior = "drop"
	OutputAzureDataExplorerBackpressureBehaviorQueue OutputAzureDataExplorerBackpressureBehavior = "queue"
)

func (e OutputAzureDataExplorerBackpressureBehavior) ToPointer() *OutputAzureDataExplorerBackpressureBehavior {
	return &e
}
func (e *OutputAzureDataExplorerBackpressureBehavior) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		fallthrough
	case "queue":
		*e = OutputAzureDataExplorerBackpressureBehavior(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerBackpressureBehavior: %v", v)
	}
}

// OutputAzureDataExplorerDataFormat - Format of the output data
type OutputAzureDataExplorerDataFormat string

const (
	OutputAzureDataExplorerDataFormatJSON    OutputAzureDataExplorerDataFormat = "json"
	OutputAzureDataExplorerDataFormatRaw     OutputAzureDataExplorerDataFormat = "raw"
	OutputAzureDataExplorerDataFormatParquet OutputAzureDataExplorerDataFormat = "parquet"
)

func (e OutputAzureDataExplorerDataFormat) ToPointer() *OutputAzureDataExplorerDataFormat {
	return &e
}
func (e *OutputAzureDataExplorerDataFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "json":
		fallthrough
	case "raw":
		fallthrough
	case "parquet":
		*e = OutputAzureDataExplorerDataFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerDataFormat: %v", v)
	}
}

// OutputAzureDataExplorerDiskSpaceProtection - Whether to block or drop events when disk space is below the global 'Min free disk space' limit
type OutputAzureDataExplorerDiskSpaceProtection string

const (
	OutputAzureDataExplorerDiskSpaceProtectionBlock OutputAzureDataExplorerDiskSpaceProtection = "block"
	OutputAzureDataExplorerDiskSpaceProtectionDrop  OutputAzureDataExplorerDiskSpaceProtection = "drop"
)

func (e OutputAzureDataExplorerDiskSpaceProtection) ToPointer() *OutputAzureDataExplorerDiskSpaceProtection {
	return &e
}
func (e *OutputAzureDataExplorerDiskSpaceProtection) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputAzureDataExplorerDiskSpaceProtection(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerDiskSpaceProtection: %v", v)
	}
}

type PrefixOptional string

const (
	PrefixOptionalDropBy   PrefixOptional = "dropBy"
	PrefixOptionalIngestBy PrefixOptional = "ingestBy"
)

func (e PrefixOptional) ToPointer() *PrefixOptional {
	return &e
}
func (e *PrefixOptional) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "dropBy":
		fallthrough
	case "ingestBy":
		*e = PrefixOptional(v)
		return nil
	default:
		return fmt.Errorf("invalid value for PrefixOptional: %v", v)
	}
}

type ExtentTags struct {
	Prefix *PrefixOptional `json:"prefix,omitempty"`
	Value  string          `json:"value"`
}

func (o *ExtentTags) GetPrefix() *PrefixOptional {
	if o == nil {
		return nil
	}
	return o.Prefix
}

func (o *ExtentTags) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type IngestIfNotExists struct {
	Value string `json:"value"`
}

func (o *IngestIfNotExists) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

// ReportLevel - Level of ingestion status reporting. Defaults to FailuresOnly.
type ReportLevel string

const (
	ReportLevelFailuresOnly         ReportLevel = "failuresOnly"
	ReportLevelDoNotReport          ReportLevel = "doNotReport"
	ReportLevelFailuresAndSuccesses ReportLevel = "failuresAndSuccesses"
)

func (e ReportLevel) ToPointer() *ReportLevel {
	return &e
}
func (e *ReportLevel) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "failuresOnly":
		fallthrough
	case "doNotReport":
		fallthrough
	case "failuresAndSuccesses":
		*e = ReportLevel(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ReportLevel: %v", v)
	}
}

// ReportMethod - Target of the ingestion status reporting. Defaults to Queue.
type ReportMethod string

const (
	ReportMethodQueue         ReportMethod = "queue"
	ReportMethodTable         ReportMethod = "table"
	ReportMethodQueueAndTable ReportMethod = "queueAndTable"
)

func (e ReportMethod) ToPointer() *ReportMethod {
	return &e
}
func (e *ReportMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "queue":
		fallthrough
	case "table":
		fallthrough
	case "queueAndTable":
		*e = ReportMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for ReportMethod: %v", v)
	}
}

type AdditionalProperties struct {
	Key   string `json:"key"`
	Value string `json:"value"`
}

func (o *AdditionalProperties) GetKey() string {
	if o == nil {
		return ""
	}
	return o.Key
}

func (o *AdditionalProperties) GetValue() string {
	if o == nil {
		return ""
	}
	return o.Value
}

type OutputAzureDataExplorerResponseRetrySettings struct {
	// The HTTP response status code that will trigger retries
	HTTPStatus float64 `json:"httpStatus"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputAzureDataExplorerResponseRetrySettings) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureDataExplorerResponseRetrySettings) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureDataExplorerResponseRetrySettings) GetHTTPStatus() float64 {
	if o == nil {
		return 0.0
	}
	return o.HTTPStatus
}

func (o *OutputAzureDataExplorerResponseRetrySettings) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputAzureDataExplorerResponseRetrySettings) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputAzureDataExplorerResponseRetrySettings) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

type OutputAzureDataExplorerTimeoutRetrySettings struct {
	// Enable to retry on request timeout
	TimeoutRetry *bool `default:"false" json:"timeoutRetry"`
	// How long, in milliseconds, Cribl Stream should wait before initiating backoff. Maximum interval is 600,000 ms (10 minutes).
	InitialBackoff *float64 `default:"1000" json:"initialBackoff"`
	// Base for exponential backoff. A value of 2 (default) means Cribl Stream will retry after 2 seconds, then 4 seconds, then 8 seconds, etc.
	BackoffRate *float64 `default:"2" json:"backoffRate"`
	// The maximum backoff interval, in milliseconds, Cribl Stream should apply. Default (and minimum) is 10,000 ms (10 seconds); maximum is 180,000 ms (180 seconds).
	MaxBackoff *float64 `default:"10000" json:"maxBackoff"`
}

func (o OutputAzureDataExplorerTimeoutRetrySettings) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureDataExplorerTimeoutRetrySettings) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureDataExplorerTimeoutRetrySettings) GetTimeoutRetry() *bool {
	if o == nil {
		return nil
	}
	return o.TimeoutRetry
}

func (o *OutputAzureDataExplorerTimeoutRetrySettings) GetInitialBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.InitialBackoff
}

func (o *OutputAzureDataExplorerTimeoutRetrySettings) GetBackoffRate() *float64 {
	if o == nil {
		return nil
	}
	return o.BackoffRate
}

func (o *OutputAzureDataExplorerTimeoutRetrySettings) GetMaxBackoff() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxBackoff
}

// OutputAzureDataExplorerCompress - Choose data compression format to apply to HTTP content before it is delivered.
type OutputAzureDataExplorerCompress string

const (
	OutputAzureDataExplorerCompressNone OutputAzureDataExplorerCompress = "none"
	OutputAzureDataExplorerCompressGzip OutputAzureDataExplorerCompress = "gzip"
)

func (e OutputAzureDataExplorerCompress) ToPointer() *OutputAzureDataExplorerCompress {
	return &e
}
func (e *OutputAzureDataExplorerCompress) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputAzureDataExplorerCompress(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerCompress: %v", v)
	}
}

// OutputAzureDataExplorerCompression - Codec to use to compress the persisted data.
type OutputAzureDataExplorerCompression string

const (
	OutputAzureDataExplorerCompressionNone OutputAzureDataExplorerCompression = "none"
	OutputAzureDataExplorerCompressionGzip OutputAzureDataExplorerCompression = "gzip"
)

func (e OutputAzureDataExplorerCompression) ToPointer() *OutputAzureDataExplorerCompression {
	return &e
}
func (e *OutputAzureDataExplorerCompression) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputAzureDataExplorerCompression(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerCompression: %v", v)
	}
}

// OutputAzureDataExplorerQueueFullBehavior - Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
type OutputAzureDataExplorerQueueFullBehavior string

const (
	OutputAzureDataExplorerQueueFullBehaviorBlock OutputAzureDataExplorerQueueFullBehavior = "block"
	OutputAzureDataExplorerQueueFullBehaviorDrop  OutputAzureDataExplorerQueueFullBehavior = "drop"
)

func (e OutputAzureDataExplorerQueueFullBehavior) ToPointer() *OutputAzureDataExplorerQueueFullBehavior {
	return &e
}
func (e *OutputAzureDataExplorerQueueFullBehavior) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputAzureDataExplorerQueueFullBehavior(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerQueueFullBehavior: %v", v)
	}
}

// OutputAzureDataExplorerMode - In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
type OutputAzureDataExplorerMode string

const (
	OutputAzureDataExplorerModeError        OutputAzureDataExplorerMode = "error"
	OutputAzureDataExplorerModeBackpressure OutputAzureDataExplorerMode = "backpressure"
	OutputAzureDataExplorerModeAlways       OutputAzureDataExplorerMode = "always"
)

func (e OutputAzureDataExplorerMode) ToPointer() *OutputAzureDataExplorerMode {
	return &e
}
func (e *OutputAzureDataExplorerMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "error":
		fallthrough
	case "backpressure":
		fallthrough
	case "always":
		*e = OutputAzureDataExplorerMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureDataExplorerMode: %v", v)
	}
}

type OutputAzureDataExplorerPqControls struct {
}

type OutputAzureDataExplorer struct {
	// Unique ID for this output
	ID   *string                      `json:"id,omitempty"`
	Type *OutputAzureDataExplorerType `json:"type,omitempty"`
	// Pipeline to process data before sending out to this output
	Pipeline *string `json:"pipeline,omitempty"`
	// Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
	SystemFields []string `json:"systemFields,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Tags for filtering and grouping in @{product}
	Streamtags []string `json:"streamtags,omitempty"`
	// The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.
	ClusterURL string `json:"clusterUrl"`
	// Name of the database containing the table where data will be ingested.
	Database string `json:"database"`
	// Name of the table to ingest data into.
	Table string `json:"table"`
	// When you save or start the Destination, validates database name and credentials; also validates table name except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.
	ValidateDatabaseSettings *bool `default:"true" json:"validateDatabaseSettings"`
	// Method to use for ingesting data.
	IngestMode *IngestionMode `default:"batching" json:"ingestMode"`
	// Endpoint used to acquire authentication tokens from Azure.
	OauthEndpoint *AzureADAuthenticationEndpoint `default:"https://login.microsoftonline.com" json:"oauthEndpoint"`
	// Directory ID (tenant identifier) in Azure Active Directory.
	TenantID string `json:"tenantId"`
	// client_id to pass in the OAuth request parameter.
	ClientID string `json:"clientId"`
	// Scope to pass in the OAuth request parameter.
	Scope string `json:"scope"`
	// The type of OAuth 2.0 client credentials grant flow to use.
	OauthType   *OutputAzureDataExplorerAuthenticationMethod `default:"clientSecret" json:"oauthType"`
	Description *string                                      `json:"description,omitempty"`
	// The client secret that you generated for your app in the Azure portal.
	ClientSecret *string `json:"clientSecret,omitempty"`
	// Select or create a stored text secret
	TextSecret  *string                             `json:"textSecret,omitempty"`
	Certificate *OutputAzureDataExplorerCertificate `json:"certificate,omitempty"`
	// The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.
	IngestURL *string `json:"ingestUrl,omitempty"`
	// Whether to block, drop, or queue events when all receivers are exerting backpressure.
	OnBackpressure *OutputAzureDataExplorerBackpressureBehavior `default:"block" json:"onBackpressure"`
	// Enable if you want to send a (JSON) mapping object instead of specifying an existing named data mapping.
	IsMappingObj *bool `default:"false" json:"isMappingObj"`
	// Format of the output data
	Format *OutputAzureDataExplorerDataFormat `default:"json" json:"format"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath *string `default:"\\$CRIBL_HOME/state/outputs/staging" json:"stagePath"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
	FileNameSuffix *string `default:".\\${C.env[\"CRIBL_WORKER_ID\"]}.\\${__format}\\${__compression === \"gzip\" ? \".gz\" : \"\"}" json:"fileNameSuffix"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *float64 `default:"32" json:"maxFileSizeMB"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *float64 `default:"300" json:"maxFileOpenTimeSec"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *float64 `default:"30" json:"maxFileIdleTimeSec"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *float64 `default:"100" json:"maxOpenFiles"`
	// Maximum number of parts to upload in parallel per file.
	MaxConcurrentFileParts *float64 `default:"1" json:"maxConcurrentFileParts"`
	// Whether to block or drop events when disk space is below the global 'Min free disk space' limit
	OnDiskFullBackpressure *OutputAzureDataExplorerDiskSpaceProtection `default:"block" json:"onDiskFullBackpressure"`
	// Append output's ID to staging location
	AddIDToStagePath *bool `default:"true" json:"addIdToStagePath"`
	// Remove empty staging directories after moving files
	RemoveEmptyDirs *bool `default:"true" json:"removeEmptyDirs"`
	// If a file fails to move to its final destination after the maximum number of retries, dead-letter it to prevent further errors.
	DeadletterEnabled *bool `default:"false" json:"deadletterEnabled"`
	// Amount of time, in seconds, to wait for a request to complete before canceling it
	TimeoutSec *float64 `default:"30" json:"timeoutSec"`
	// Enable to bypass the data management service's aggregation mechanism.
	FlushImmediately *bool `default:"false" json:"flushImmediately"`
	// Enable to prevent blob deletion after ingestion is complete.
	RetainBlobOnSuccess *bool `default:"false" json:"retainBlobOnSuccess"`
	// Strings or tags associated with the extent (ingested data shard).
	ExtentTags []ExtentTags `json:"extentTags,omitempty"`
	// Prevents duplicate ingestion by checking if an extent with the specified ingest-by tag already exists.
	IngestIfNotExists []IngestIfNotExists `json:"ingestIfNotExists,omitempty"`
	// Level of ingestion status reporting. Defaults to FailuresOnly.
	ReportLevel *ReportLevel `default:"failuresOnly" json:"reportLevel"`
	// Target of the ingestion status reporting. Defaults to Queue.
	ReportMethod *ReportMethod `default:"queue" json:"reportMethod"`
	// Optionally, enter additional configuration properties to send to the ingestion service.
	AdditionalProperties []AdditionalProperties `json:"additionalProperties,omitempty"`
	// Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable).
	ResponseRetrySettings []OutputAzureDataExplorerResponseRetrySettings `json:"responseRetrySettings,omitempty"`
	TimeoutRetrySettings  *OutputAzureDataExplorerTimeoutRetrySettings   `json:"timeoutRetrySettings,omitempty"`
	// Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
	ResponseHonorRetryAfterHeader *bool `default:"false" json:"responseHonorRetryAfterHeader"`
	// Choose data compression format to apply to HTTP content before it is delivered.
	Compress *OutputAzureDataExplorerCompress `default:"gzip" json:"compress"`
	// Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.
	MappingRef *string `json:"mappingRef,omitempty"`
	// Maximum number of ongoing requests before blocking
	Concurrency *float64 `default:"5" json:"concurrency"`
	// Maximum size, in KB, of the request body
	MaxPayloadSizeKB *float64 `default:"4096" json:"maxPayloadSizeKB"`
	// Maximum number of events to include in the request body. Default is 0 (unlimited).
	MaxPayloadEvents *float64 `default:"0" json:"maxPayloadEvents"`
	// Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max body size.
	FlushPeriodSec *float64 `default:"1" json:"flushPeriodSec"`
	// Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
	//         Defaults to Yes. When this setting is also present in TLS Settings (Client Side),
	//         that value will take precedence.
	RejectUnauthorized *bool `default:"true" json:"rejectUnauthorized"`
	// Enables round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
	UseRoundRobinDNS *bool `default:"false" json:"useRoundRobinDns"`
	// Disable to close the connection immediately after sending the outgoing request.
	KeepAlive *bool `default:"true" json:"keepAlive"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	PqMaxFileSize *string `default:"1 MB" json:"pqMaxFileSize"`
	// The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
	PqMaxSize *string `default:"5GB" json:"pqMaxSize"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
	PqPath *string `default:"\\$CRIBL_HOME/state/queues" json:"pqPath"`
	// Codec to use to compress the persisted data.
	PqCompress *OutputAzureDataExplorerCompression `default:"none" json:"pqCompress"`
	// Whether to block or drop events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
	PqOnBackpressure *OutputAzureDataExplorerQueueFullBehavior `default:"block" json:"pqOnBackpressure"`
	// In Error mode, PQ writes events to the filesystem only when it detects a non-retryable Destination error. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination or when there are non-retryable Destination errors. In Always On mode, PQ always writes events to the filesystem.
	PqMode     *OutputAzureDataExplorerMode       `default:"error" json:"pqMode"`
	PqControls *OutputAzureDataExplorerPqControls `json:"pqControls,omitempty"`
}

func (o OutputAzureDataExplorer) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(o, "", false)
}

func (o *OutputAzureDataExplorer) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &o, "", false, true); err != nil {
		return err
	}
	return nil
}

func (o *OutputAzureDataExplorer) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *OutputAzureDataExplorer) GetType() *OutputAzureDataExplorerType {
	if o == nil {
		return nil
	}
	return o.Type
}

func (o *OutputAzureDataExplorer) GetPipeline() *string {
	if o == nil {
		return nil
	}
	return o.Pipeline
}

func (o *OutputAzureDataExplorer) GetSystemFields() []string {
	if o == nil {
		return nil
	}
	return o.SystemFields
}

func (o *OutputAzureDataExplorer) GetEnvironment() *string {
	if o == nil {
		return nil
	}
	return o.Environment
}

func (o *OutputAzureDataExplorer) GetStreamtags() []string {
	if o == nil {
		return nil
	}
	return o.Streamtags
}

func (o *OutputAzureDataExplorer) GetClusterURL() string {
	if o == nil {
		return ""
	}
	return o.ClusterURL
}

func (o *OutputAzureDataExplorer) GetDatabase() string {
	if o == nil {
		return ""
	}
	return o.Database
}

func (o *OutputAzureDataExplorer) GetTable() string {
	if o == nil {
		return ""
	}
	return o.Table
}

func (o *OutputAzureDataExplorer) GetValidateDatabaseSettings() *bool {
	if o == nil {
		return nil
	}
	return o.ValidateDatabaseSettings
}

func (o *OutputAzureDataExplorer) GetIngestMode() *IngestionMode {
	if o == nil {
		return nil
	}
	return o.IngestMode
}

func (o *OutputAzureDataExplorer) GetOauthEndpoint() *AzureADAuthenticationEndpoint {
	if o == nil {
		return nil
	}
	return o.OauthEndpoint
}

func (o *OutputAzureDataExplorer) GetTenantID() string {
	if o == nil {
		return ""
	}
	return o.TenantID
}

func (o *OutputAzureDataExplorer) GetClientID() string {
	if o == nil {
		return ""
	}
	return o.ClientID
}

func (o *OutputAzureDataExplorer) GetScope() string {
	if o == nil {
		return ""
	}
	return o.Scope
}

func (o *OutputAzureDataExplorer) GetOauthType() *OutputAzureDataExplorerAuthenticationMethod {
	if o == nil {
		return nil
	}
	return o.OauthType
}

func (o *OutputAzureDataExplorer) GetDescription() *string {
	if o == nil {
		return nil
	}
	return o.Description
}

func (o *OutputAzureDataExplorer) GetClientSecret() *string {
	if o == nil {
		return nil
	}
	return o.ClientSecret
}

func (o *OutputAzureDataExplorer) GetTextSecret() *string {
	if o == nil {
		return nil
	}
	return o.TextSecret
}

func (o *OutputAzureDataExplorer) GetCertificate() *OutputAzureDataExplorerCertificate {
	if o == nil {
		return nil
	}
	return o.Certificate
}

func (o *OutputAzureDataExplorer) GetIngestURL() *string {
	if o == nil {
		return nil
	}
	return o.IngestURL
}

func (o *OutputAzureDataExplorer) GetOnBackpressure() *OutputAzureDataExplorerBackpressureBehavior {
	if o == nil {
		return nil
	}
	return o.OnBackpressure
}

func (o *OutputAzureDataExplorer) GetIsMappingObj() *bool {
	if o == nil {
		return nil
	}
	return o.IsMappingObj
}

func (o *OutputAzureDataExplorer) GetFormat() *OutputAzureDataExplorerDataFormat {
	if o == nil {
		return nil
	}
	return o.Format
}

func (o *OutputAzureDataExplorer) GetStagePath() *string {
	if o == nil {
		return nil
	}
	return o.StagePath
}

func (o *OutputAzureDataExplorer) GetFileNameSuffix() *string {
	if o == nil {
		return nil
	}
	return o.FileNameSuffix
}

func (o *OutputAzureDataExplorer) GetMaxFileSizeMB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileSizeMB
}

func (o *OutputAzureDataExplorer) GetMaxFileOpenTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileOpenTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxFileIdleTimeSec() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxFileIdleTimeSec
}

func (o *OutputAzureDataExplorer) GetMaxOpenFiles() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxOpenFiles
}

func (o *OutputAzureDataExplorer) GetMaxConcurrentFileParts() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxConcurrentFileParts
}

func (o *OutputAzureDataExplorer) GetOnDiskFullBackpressure() *OutputAzureDataExplorerDiskSpaceProtection {
	if o == nil {
		return nil
	}
	return o.OnDiskFullBackpressure
}

func (o *OutputAzureDataExplorer) GetAddIDToStagePath() *bool {
	if o == nil {
		return nil
	}
	return o.AddIDToStagePath
}

func (o *OutputAzureDataExplorer) GetRemoveEmptyDirs() *bool {
	if o == nil {
		return nil
	}
	return o.RemoveEmptyDirs
}

func (o *OutputAzureDataExplorer) GetDeadletterEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.DeadletterEnabled
}

func (o *OutputAzureDataExplorer) GetTimeoutSec() *float64 {
	if o == nil {
		return nil
	}
	return o.TimeoutSec
}

func (o *OutputAzureDataExplorer) GetFlushImmediately() *bool {
	if o == nil {
		return nil
	}
	return o.FlushImmediately
}

func (o *OutputAzureDataExplorer) GetRetainBlobOnSuccess() *bool {
	if o == nil {
		return nil
	}
	return o.RetainBlobOnSuccess
}

func (o *OutputAzureDataExplorer) GetExtentTags() []ExtentTags {
	if o == nil {
		return nil
	}
	return o.ExtentTags
}

func (o *OutputAzureDataExplorer) GetIngestIfNotExists() []IngestIfNotExists {
	if o == nil {
		return nil
	}
	return o.IngestIfNotExists
}

func (o *OutputAzureDataExplorer) GetReportLevel() *ReportLevel {
	if o == nil {
		return nil
	}
	return o.ReportLevel
}

func (o *OutputAzureDataExplorer) GetReportMethod() *ReportMethod {
	if o == nil {
		return nil
	}
	return o.ReportMethod
}

func (o *OutputAzureDataExplorer) GetAdditionalProperties() []AdditionalProperties {
	if o == nil {
		return nil
	}
	return o.AdditionalProperties
}

func (o *OutputAzureDataExplorer) GetResponseRetrySettings() []OutputAzureDataExplorerResponseRetrySettings {
	if o == nil {
		return nil
	}
	return o.ResponseRetrySettings
}

func (o *OutputAzureDataExplorer) GetTimeoutRetrySettings() *OutputAzureDataExplorerTimeoutRetrySettings {
	if o == nil {
		return nil
	}
	return o.TimeoutRetrySettings
}

func (o *OutputAzureDataExplorer) GetResponseHonorRetryAfterHeader() *bool {
	if o == nil {
		return nil
	}
	return o.ResponseHonorRetryAfterHeader
}

func (o *OutputAzureDataExplorer) GetCompress() *OutputAzureDataExplorerCompress {
	if o == nil {
		return nil
	}
	return o.Compress
}

func (o *OutputAzureDataExplorer) GetMappingRef() *string {
	if o == nil {
		return nil
	}
	return o.MappingRef
}

func (o *OutputAzureDataExplorer) GetConcurrency() *float64 {
	if o == nil {
		return nil
	}
	return o.Concurrency
}

func (o *OutputAzureDataExplorer) GetMaxPayloadSizeKB() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadSizeKB
}

func (o *OutputAzureDataExplorer) GetMaxPayloadEvents() *float64 {
	if o == nil {
		return nil
	}
	return o.MaxPayloadEvents
}

func (o *OutputAzureDataExplorer) GetFlushPeriodSec() *float64 {
	if o == nil {
		return nil
	}
	return o.FlushPeriodSec
}

func (o *OutputAzureDataExplorer) GetRejectUnauthorized() *bool {
	if o == nil {
		return nil
	}
	return o.RejectUnauthorized
}

func (o *OutputAzureDataExplorer) GetUseRoundRobinDNS() *bool {
	if o == nil {
		return nil
	}
	return o.UseRoundRobinDNS
}

func (o *OutputAzureDataExplorer) GetKeepAlive() *bool {
	if o == nil {
		return nil
	}
	return o.KeepAlive
}

func (o *OutputAzureDataExplorer) GetPqMaxFileSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxFileSize
}

func (o *OutputAzureDataExplorer) GetPqMaxSize() *string {
	if o == nil {
		return nil
	}
	return o.PqMaxSize
}

func (o *OutputAzureDataExplorer) GetPqPath() *string {
	if o == nil {
		return nil
	}
	return o.PqPath
}

func (o *OutputAzureDataExplorer) GetPqCompress() *OutputAzureDataExplorerCompression {
	if o == nil {
		return nil
	}
	return o.PqCompress
}

func (o *OutputAzureDataExplorer) GetPqOnBackpressure() *OutputAzureDataExplorerQueueFullBehavior {
	if o == nil {
		return nil
	}
	return o.PqOnBackpressure
}

func (o *OutputAzureDataExplorer) GetPqMode() *OutputAzureDataExplorerMode {
	if o == nil {
		return nil
	}
	return o.PqMode
}

func (o *OutputAzureDataExplorer) GetPqControls() *OutputAzureDataExplorerPqControls {
	if o == nil {
		return nil
	}
	return o.PqControls
}
